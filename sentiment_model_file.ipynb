{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WV_JKOaAA4u6",
        "outputId": "ed6ed78a-0b66-4bc6-da9a-cba536e177ed"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/devarsh.patel/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/devarsh.patel/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# import packages\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM,Dense, Dropout, SpatialDropout1D\n",
        "from tensorflow.keras.layers import Embedding,BatchNormalization\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "import re\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "sns.set()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "90-EdBfpA9M3"
      },
      "outputs": [],
      "source": [
        "#import dataset and clean dataset\n",
        "df = pd.read_csv('tweet_sentiment.csv')\n",
        "df.drop_duplicates(inplace=True)\n",
        "df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "c9r-8G0TFpn7"
      },
      "outputs": [],
      "source": [
        "#function to remove punctutaions \n",
        "def remove_punch(txt):\n",
        "    txt_punch_removed = [char for char in txt if char not in string.punctuation ]\n",
        "    return ''.join(txt_punch_removed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "QdpT6kD_Fp_0"
      },
      "outputs": [],
      "source": [
        "#clena the text\n",
        "df[\"cleaned_tweets\"]=df[\"cleaned_tweets\"].str.lower()\n",
        "df[\"cleaned_tweets\"] = df[\"cleaned_tweets\"].apply(remove_punch)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df['cleaned_tweets'] = df['cleaned_tweets'].apply(lambda sent: [word for word in word_tokenize(sent) if word not in stop_words])\n",
        "stemmer = PorterStemmer()\n",
        "df['cleaned_tweets'] = df['cleaned_tweets'].apply(lambda words: [stemmer.stem(word) for word in words])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "68PNEySzCZqX"
      },
      "outputs": [],
      "source": [
        "#dictionary to track the words from vocab\n",
        "def get_dict(x:list):\n",
        "    indx_count = 1\n",
        "    word_to_indx = {}\n",
        "    indx_to_word = {}\n",
        "    for sent in x:\n",
        "        for word in sent:\n",
        "            if word not in word_to_indx.keys():\n",
        "                word_to_indx[word] = indx_count\n",
        "                indx_to_word[indx_count] = word\n",
        "                indx_count +=1\n",
        "    return word_to_indx, indx_to_word\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ZdeUkY_YCiGH"
      },
      "outputs": [],
      "source": [
        "word_to_indx, indx_to_word = get_dict(df[\"cleaned_tweets\"])\n",
        "volc_size = len(word_to_indx.keys())\n",
        "volc_size += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json \n",
        "       \n",
        "      \n",
        "# Serializing json  \n",
        "with open(\"sample.json\", \"w\") as outfile:\n",
        "    json.dump(word_to_indx, outfile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "T00D0WSSDwQH"
      },
      "outputs": [],
      "source": [
        "#converting textual data to numerical\n",
        "input_size = 50\n",
        "df[\"cleaned_tweets\"] = df[\"cleaned_tweets\"].apply(lambda words: [word_to_indx[word] for word in words])\n",
        "data_x = pad_sequences(df[\"cleaned_tweets\"], maxlen=input_size ,dtype=\"object\", padding=\"post\", truncating=\"post\")\n",
        "data_x = data_x.astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "oadziqnRD3z1"
      },
      "outputs": [],
      "source": [
        "data_y = pd.get_dummies(df[\"sentiment\"],drop_first=False).values\n",
        "data_y = data_y.astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "majDlokMD6qb"
      },
      "outputs": [],
      "source": [
        "#split of data to train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(data_x, data_y, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkYwalT3D8qa",
        "outputId": "a20231ed-f68a-47f8-f406-4801f0c11f0e"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "add() got an unexpected keyword argument 'return_sequences'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/Users/devarsh.patel/Documents/Scripts/Sentiment Analysis/sentiment_model_file.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/devarsh.patel/Documents/Scripts/Sentiment%20Analysis/sentiment_model_file.ipynb#ch0000009?line=0'>1</a>\u001b[0m \u001b[39m#building model architecture\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/devarsh.patel/Documents/Scripts/Sentiment%20Analysis/sentiment_model_file.ipynb#ch0000009?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m Sequential()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/devarsh.patel/Documents/Scripts/Sentiment%20Analysis/sentiment_model_file.ipynb#ch0000009?line=2'>3</a>\u001b[0m model\u001b[39m.\u001b[39;49madd(Embedding(volc_size, \u001b[39m32\u001b[39;49m, input_length\u001b[39m=\u001b[39;49minput_size), return_sequences\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/devarsh.patel/Documents/Scripts/Sentiment%20Analysis/sentiment_model_file.ipynb#ch0000009?line=3'>4</a>\u001b[0m model\u001b[39m.\u001b[39madd(SpatialDropout1D(\u001b[39m0.2\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/devarsh.patel/Documents/Scripts/Sentiment%20Analysis/sentiment_model_file.ipynb#ch0000009?line=4'>5</a>\u001b[0m model\u001b[39m.\u001b[39madd(LSTM(\u001b[39m128\u001b[39m, dropout\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, recurrent_dropout\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m))\n",
            "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/training/tracking/base.py:587\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    586\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 587\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    588\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    589\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "\u001b[0;31mTypeError\u001b[0m: add() got an unexpected keyword argument 'return_sequences'"
          ]
        }
      ],
      "source": [
        "#building model architecture\n",
        "model = Sequential()\n",
        "model.add(Embedding(volc_size, 32, input_length=input_size))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(2048,activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(3,activation='softmax'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnPwCw6kD-Cr",
        "outputId": "b92b9efd-3f27-43c2-9f0a-c06db82fccd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "440/440 [==============================] - 31s 64ms/step - loss: 1.0363 - accuracy: 0.5549 - val_loss: 0.9130 - val_accuracy: 0.5911\n",
            "Epoch 2/10\n",
            "440/440 [==============================] - 33s 75ms/step - loss: 0.9043 - accuracy: 0.5983 - val_loss: 0.9897 - val_accuracy: 0.3140\n",
            "Epoch 3/10\n",
            "440/440 [==============================] - 34s 78ms/step - loss: 0.8996 - accuracy: 0.6001 - val_loss: 0.9500 - val_accuracy: 0.5911\n",
            "Epoch 4/10\n",
            "440/440 [==============================] - 38s 86ms/step - loss: 0.8995 - accuracy: 0.5987 - val_loss: 0.9378 - val_accuracy: 0.5911\n",
            "Epoch 5/10\n",
            "440/440 [==============================] - 41s 93ms/step - loss: 0.8998 - accuracy: 0.6001 - val_loss: 1.0295 - val_accuracy: 0.5911\n",
            "Epoch 6/10\n",
            "440/440 [==============================] - 44s 100ms/step - loss: 0.8983 - accuracy: 0.6002 - val_loss: 0.9960 - val_accuracy: 0.5911\n",
            "Epoch 7/10\n",
            "440/440 [==============================] - 44s 101ms/step - loss: 0.8990 - accuracy: 0.6001 - val_loss: 0.9328 - val_accuracy: 0.5911\n",
            "Epoch 8/10\n",
            "440/440 [==============================] - 41s 93ms/step - loss: 0.8991 - accuracy: 0.6002 - val_loss: 1.2726 - val_accuracy: 0.0949\n",
            "Epoch 9/10\n",
            "440/440 [==============================] - 41s 93ms/step - loss: 0.9007 - accuracy: 0.5994 - val_loss: 0.9584 - val_accuracy: 0.5911\n",
            "Epoch 10/10\n",
            "440/440 [==============================] - 41s 93ms/step - loss: 0.8993 - accuracy: 0.5993 - val_loss: 0.9049 - val_accuracy: 0.5911\n"
          ]
        }
      ],
      "source": [
        "#fitting model on train data\n",
        "h = model.fit(X_train,y_train, validation_split=0.2, epochs=10, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "_Zs6YwOzEAk0"
      },
      "outputs": [],
      "source": [
        "#predict the sentiment on test\n",
        "def pos_neg(y_pred):\n",
        "  dec=''\n",
        "  if np.argmax(y_pred)==0:\n",
        "    dec='negative'\n",
        "  elif np.argmax(y_pred)==1:\n",
        "    dec='neutral'\n",
        "  else:\n",
        "    dec='positive'\n",
        "  return dec\n",
        "\n",
        "\n",
        "def predict(data):\n",
        "  y_pred=model.predict(data)\n",
        "  dec=pos_neg(y_pred)\n",
        "  return dec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-4LEC81XcOk",
        "outputId": "f1ca0591-0531-4c1c-db38-836479fb15d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ram://92c9dd54-6151-4e48-bf70-7d2dea3d14aa/assets\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "pickle.dump(model, open('model.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "2ziHS_3YYC7C"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'negative'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "sentiment_model_file.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
